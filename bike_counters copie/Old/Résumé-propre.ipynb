{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "data = pd.read_parquet(Path(\"data\") / \"train.parquet\")\n",
    "external_data = pd.read_csv('submissions/external_data/external_data.csv')\n",
    "\n",
    "def reinit():\n",
    "    return(pd.read_parquet(Path(\"data\") / \"train.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "exd = external_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_dates(X):\n",
    "    X = X.copy()  # modify a copy of X\n",
    "    # Encode the date information from the DateOfDeparture columns\n",
    "    X.loc[:, \"year\"] = X[\"date\"].dt.year\n",
    "    X.loc[:, \"month\"] = X[\"date\"].dt.month\n",
    "    X.loc[:, \"day\"] = X[\"date\"].dt.day\n",
    "    X.loc[:, \"weekday\"] = X[\"date\"].dt.weekday\n",
    "    X.loc[:, \"hour\"] = X[\"date\"].dt.hour\n",
    "\n",
    "    # Finally we can drop the original columns from the dataframe\n",
    "    return X.drop(columns=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cyclic_hours(X):\n",
    "    X = X.copy()# modify a copy of X\n",
    "    # Encode the date information from the DateOfDeparture columns\n",
    "    X.loc[:, \"hour_c\"] = np.cos(np.pi/12 * X[\"hour\"])\n",
    "    X.loc[:, \"hour_s\"] = np.sin(np.pi/12 * X[\"hour\"])\n",
    "    # a way to show the cyclical ways of the hours\n",
    "    return(X.drop(columns=\"hour\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_int_ex(X, external):\n",
    "    \"\"\" Join X and Y on the same time period, with periods within external\"\"\"\n",
    "    external = external.copy()\n",
    "    X = X.copy()\n",
    "    \n",
    "    X.loc[:, \"year\"] = X[\"date\"].dt.year\n",
    "    X.loc[:, \"month\"] = X[\"date\"].dt.month\n",
    "    X.loc[:, \"day\"] = X[\"date\"].dt.day\n",
    "    X.loc[:, \"time_period\"] = X[\"date\"].dt.hour //3\n",
    "\n",
    "    external['date'] = pd.to_datetime(exd['date'])\n",
    "    external.loc[:, \"year\"] = external[\"date\"].dt.year\n",
    "    external.loc[:, \"month\"] = external[\"date\"].dt.month\n",
    "    external.loc[:, \"day\"] = external[\"date\"].dt.day\n",
    "    external.loc[:, \"time_period\"] = external[\"date\"].dt.hour //3\n",
    "    external.drop(columns=\"date\", inplace=True)\n",
    "\n",
    "    u = X.set_index(['year', 'month', 'day', 'time_period'])\n",
    "    v = external.set_index(['year', 'month', 'day', 'time_period'])\n",
    "    w = u.join(v).reset_index()\n",
    "    w.drop(columns=['year', 'month', 'day'], inplace=True)\n",
    "\n",
    "    return(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colones à garder pour la fusion dans la external data. Critères =  \n",
    "#   - les données logiques sont inclues dedans\n",
    "#   - tous ces indices procurent au plus 0.4% de cellules vides (correspond à 10 cellules) \n",
    "#     et ce sont les seules\n",
    "\n",
    "good_col = [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 15, 20, 23, 31, 32, 33, 38, 39]\n",
    "\n",
    "exd = exd.iloc[:, good_col].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"counter_name\", \"site_name\", \"bike_count\", \"counter_installation_date\", 'counter_technical_id', 'coordinates'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodage de la donnée\n",
    "\n",
    "internal :\n",
    "\n",
    "counter : 1 hot encoder \\\n",
    "Site : one hot encoder ? ou evtl le drop au profit de la localisation\\\n",
    "installation date : osef normalement : n'a d'impact que sur le début (les gens découvrent la nouvelle station et adaptent leurs habitudes) ---- a rajouter dans un second temps.\\\n",
    "technical instl : osef \\\n",
    "bike count : osef, ou sinon faire du rev eng pour la target. \\\n",
    "year - month - day : commencer par un truc tel quel. on pourra rajouter les saison après pour marquer les séparations. \\\n",
    "hour : codé sur cos et sin \\\n",
    "\n",
    "external : laissé tel quel pour l'instant, avec du engeeniring plus tard. \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "union = merge_int_ex(df, exd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_period          0\n",
       "counter_id           0\n",
       "site_id              0\n",
       "date                 0\n",
       "latitude             0\n",
       "longitude            0\n",
       "log_bike_count       0\n",
       "numer_sta          666\n",
       "pmer               666\n",
       "tend               666\n",
       "cod_tend           666\n",
       "dd                 666\n",
       "ff                 666\n",
       "t                  666\n",
       "td                 666\n",
       "u                  666\n",
       "vv                 666\n",
       "ww                 666\n",
       "w1                1662\n",
       "w2                2154\n",
       "nbas              1500\n",
       "pres               666\n",
       "tend24            1998\n",
       "raf10             1998\n",
       "rafper             666\n",
       "per                666\n",
       "rr1               1506\n",
       "rr3               1170\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by removing all the na values, we lose  0.77% of the info of the total dataset.\n"
     ]
    }
   ],
   "source": [
    "n0= union.shape[0]\n",
    "df = union.copy()\n",
    "df.dropna(axis='index', inplace=True)\n",
    "n1=df.shape[0]\n",
    "\n",
    "info_loss= (1- n1/n0)*100\n",
    "\n",
    "print(f\"by removing all the na values, we lose {info_loss: .2f}% of the info of the total dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal : find which columns containing NaN we removed and we could put back in, to find the best trade off between a lot of features, and less information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7b30246996708325107b392c051508be72414558de438b0f8d2cdae0c33c670"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
